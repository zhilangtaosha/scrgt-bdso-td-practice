{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Cleansed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>period</th>\n",
       "      <th>name</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>sec_adsh</th>\n",
       "      <th>sec_tag</th>\n",
       "      <th>sec_value</th>\n",
       "      <th>sec_cik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AABA</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>Altaba Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>EDP Services</td>\n",
       "      <td>None</td>\n",
       "      <td>CompanyName | Industry</td>\n",
       "      <td>Altaba Inc. | EDP Services</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AABA</td>\n",
       "      <td>2017-03-30</td>\n",
       "      <td>Altaba Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>EDP Services</td>\n",
       "      <td>None</td>\n",
       "      <td>CompanyName | Industry</td>\n",
       "      <td>Altaba Inc. | EDP Services</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AABA</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>Altaba Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>EDP Services</td>\n",
       "      <td>None</td>\n",
       "      <td>CompanyName | Industry</td>\n",
       "      <td>Altaba Inc. | EDP Services</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AABA</td>\n",
       "      <td>2017-09-30</td>\n",
       "      <td>Altaba Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>EDP Services</td>\n",
       "      <td>None</td>\n",
       "      <td>CompanyName | Industry</td>\n",
       "      <td>Altaba Inc. | EDP Services</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AABA</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>Altaba Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>EDP Services</td>\n",
       "      <td>None</td>\n",
       "      <td>CompanyName | Industry</td>\n",
       "      <td>Altaba Inc. | EDP Services</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AABA</td>\n",
       "      <td>2018-03-30</td>\n",
       "      <td>Altaba Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>EDP Services</td>\n",
       "      <td>None</td>\n",
       "      <td>CompanyName | Industry</td>\n",
       "      <td>Altaba Inc. | EDP Services</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AABA</td>\n",
       "      <td>2018-06-30</td>\n",
       "      <td>Altaba Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>EDP Services</td>\n",
       "      <td>None</td>\n",
       "      <td>CompanyName | Industry</td>\n",
       "      <td>Altaba Inc. | EDP Services</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AABA</td>\n",
       "      <td>2018-09-30</td>\n",
       "      <td>Altaba Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>EDP Services</td>\n",
       "      <td>None</td>\n",
       "      <td>CompanyName | Industry</td>\n",
       "      <td>Altaba Inc. | EDP Services</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AABA</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>Altaba Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>EDP Services</td>\n",
       "      <td>None</td>\n",
       "      <td>CompanyName | Industry</td>\n",
       "      <td>Altaba Inc. | EDP Services</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAL</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>American Airlines Group, Inc.</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>Air Freight/Delivery Services</td>\n",
       "      <td>None</td>\n",
       "      <td>CompanyName | Industry</td>\n",
       "      <td>American Airlines Group, Inc. | Air Freight/De...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker      period                           name          sector  \\\n",
       "0   AABA  2016-12-31                    Altaba Inc.      Technology   \n",
       "1   AABA  2017-03-30                    Altaba Inc.      Technology   \n",
       "2   AABA  2017-06-30                    Altaba Inc.      Technology   \n",
       "3   AABA  2017-09-30                    Altaba Inc.      Technology   \n",
       "4   AABA  2017-12-31                    Altaba Inc.      Technology   \n",
       "5   AABA  2018-03-30                    Altaba Inc.      Technology   \n",
       "6   AABA  2018-06-30                    Altaba Inc.      Technology   \n",
       "7   AABA  2018-09-30                    Altaba Inc.      Technology   \n",
       "8   AABA  2018-12-31                    Altaba Inc.      Technology   \n",
       "9    AAL  2016-12-31  American Airlines Group, Inc.  Transportation   \n",
       "\n",
       "                        industry sec_adsh                 sec_tag  \\\n",
       "0                   EDP Services     None  CompanyName | Industry   \n",
       "1                   EDP Services     None  CompanyName | Industry   \n",
       "2                   EDP Services     None  CompanyName | Industry   \n",
       "3                   EDP Services     None  CompanyName | Industry   \n",
       "4                   EDP Services     None  CompanyName | Industry   \n",
       "5                   EDP Services     None  CompanyName | Industry   \n",
       "6                   EDP Services     None  CompanyName | Industry   \n",
       "7                   EDP Services     None  CompanyName | Industry   \n",
       "8                   EDP Services     None  CompanyName | Industry   \n",
       "9  Air Freight/Delivery Services     None  CompanyName | Industry   \n",
       "\n",
       "                                           sec_value sec_cik  \n",
       "0                         Altaba Inc. | EDP Services    None  \n",
       "1                         Altaba Inc. | EDP Services    None  \n",
       "2                         Altaba Inc. | EDP Services    None  \n",
       "3                         Altaba Inc. | EDP Services    None  \n",
       "4                         Altaba Inc. | EDP Services    None  \n",
       "5                         Altaba Inc. | EDP Services    None  \n",
       "6                         Altaba Inc. | EDP Services    None  \n",
       "7                         Altaba Inc. | EDP Services    None  \n",
       "8                         Altaba Inc. | EDP Services    None  \n",
       "9  American Airlines Group, Inc. | Air Freight/De...    None  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "con = create_engine(\n",
    "        'postgresql://postgres:Password2019@dev-test.cqfmdduunoq1.us-east-1.rds.amazonaws.com:5432/mdas_db')\n",
    "df = pd.read_sql_query('''select * from cleansed.nasdaq_txt_main''', con=con)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 1323389 items in vocab_frame\n",
      "Wall time: 31.5 s\n",
      "(31202, 495)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "#import mpld3\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# stopwords.extend(['revenue','recognition','recognize','recognized','recognizes','together','collectively','provide',\n",
    "#                   'well','companies','company','companys','service','services','product','products','commercial',\n",
    "#                   'business','customer','united','states','us','u.s.','june','january','may','february','march',\n",
    "#                   'july','august','september','november','december','april','october','corporation','corp',\n",
    "#                   'incorporated','inc.','inc','including','primarily','provides','nature','also','new','used',\n",
    "#                   'statements','consolidated','description','include','note','one','two','three','organization',\n",
    "#                   'amount', 'based', 'consideration', 'contract', 'contracts', 'customers', 'net', 'obligation', \n",
    "#                   'obligations', 'period', 'related', 'revenues', 'time', 'upon', 'basis', 'generally', \n",
    "#                   'following', 'certain', 'year','recorded'])\n",
    "stopwords.extend([\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'u.s', 'wo', 'would'])\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "dfList = list(df['sec_value'])\n",
    "\n",
    "totalvocab_tokenized = []\n",
    "for i in dfList:\n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_tokenized)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n",
    "# print(totalvocab_tokenized[:10])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500,\n",
    "                                 min_df=0.01, stop_words=stopwords,\n",
    "                                 use_idf=True, tokenizer=tokenize_only, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(dfList)\n",
    "\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "# print(terms)\n",
    "# print(len(terms))\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "# print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: holdings, fund, ltd., inc, investment,\n",
      "\n",
      "Cluster 1 words: pharmaceuticals, major, major, inc., inc.,\n",
      "\n",
      "Cluster 2 words: company, software, inc., computer, computer,\n",
      "\n",
      "Cluster 3 words: etf, invesco, ishares, first, trust,\n",
      "\n",
      "Cluster 4 words: edp, edp, inc., inc., services,\n",
      "\n",
      "Cluster 5 words: banks, major, major, inc., bancorp,\n",
      "\n",
      "Cluster 6 words: inc., services, group, group, inc.,\n",
      "\n",
      "Cluster 7 words: corporation, semiconductors, capital, industrial, investment,\n",
      "\n",
      "Cluster 8 words: biotechnology, inc., diagnostic, substances, diagnostic,\n",
      "\n",
      "Cluster 9 words: business, business, services, acquisition, corp.,\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>cluster</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sector</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Basic Industries</th>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>337</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Capital Goods</th>\n",
       "      <td>325</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>688</td>\n",
       "      <td>248</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consumer Durables</th>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consumer Non-Durables</th>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>456</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consumer Services</th>\n",
       "      <td>1059</td>\n",
       "      <td>0</td>\n",
       "      <td>469</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1331</td>\n",
       "      <td>378</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>3273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Energy</th>\n",
       "      <td>185</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>191</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finance</th>\n",
       "      <td>1165</td>\n",
       "      <td>0</td>\n",
       "      <td>911</td>\n",
       "      <td>0</td>\n",
       "      <td>2076</td>\n",
       "      <td>531</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>1456</td>\n",
       "      <td>6309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health Care</th>\n",
       "      <td>292</td>\n",
       "      <td>2968</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>790</td>\n",
       "      <td>151</td>\n",
       "      <td>1192</td>\n",
       "      <td>0</td>\n",
       "      <td>6781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miscellaneous</th>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>503</td>\n",
       "      <td>883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Public Utilities</th>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Technology</th>\n",
       "      <td>416</td>\n",
       "      <td>0</td>\n",
       "      <td>1581</td>\n",
       "      <td>762</td>\n",
       "      <td>0</td>\n",
       "      <td>868</td>\n",
       "      <td>278</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transportation</th>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4314</td>\n",
       "      <td>2968</td>\n",
       "      <td>5634</td>\n",
       "      <td>762</td>\n",
       "      <td>2076</td>\n",
       "      <td>6046</td>\n",
       "      <td>1765</td>\n",
       "      <td>1262</td>\n",
       "      <td>2001</td>\n",
       "      <td>26828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "cluster                   0     1     2    4     5     6     7     8     9  \\\n",
       "sector                                                                       \n",
       "Basic Industries         81     0   176    0     0   337    90     0     0   \n",
       "Capital Goods           325     0   299    0     0   688   248    70     0   \n",
       "Consumer Durables       173     0    92    0     0   284    81     0     0   \n",
       "Consumer Non-Durables   201     0   250    0     0   456   157     0     0   \n",
       "Consumer Services      1059     0   469    0     0  1331   378     0    36   \n",
       "Energy                  185     0   153    0     0   191    63     0     0   \n",
       "Finance                1165     0   911    0  2076   531   170     0  1456   \n",
       "Health Care             292  2968  1388    0     0   790   151  1192     0   \n",
       "Miscellaneous            59     0   172    0     0    94    55     0   503   \n",
       "Public Utilities        184     0   101    0     0   236    55     0     0   \n",
       "Technology              416     0  1581  762     0   868   278     0     6   \n",
       "Transportation          174     0    42    0     0   240    39     0     0   \n",
       "All                    4314  2968  5634  762  2076  6046  1765  1262  2001   \n",
       "\n",
       "cluster                  All  \n",
       "sector                        \n",
       "Basic Industries         684  \n",
       "Capital Goods           1630  \n",
       "Consumer Durables        630  \n",
       "Consumer Non-Durables   1064  \n",
       "Consumer Services       3273  \n",
       "Energy                   592  \n",
       "Finance                 6309  \n",
       "Health Care             6781  \n",
       "Miscellaneous            883  \n",
       "Public Utilities         576  \n",
       "Technology              3911  \n",
       "Transportation           495  \n",
       "All                    26828  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from clustering import create_cluster\n",
    "# df_index, df = create_cluster(tfidf_matrix, df, vocab_frame)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 10\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "df['cluster'] = clusters\n",
    "df_index = df.copy()\n",
    "df_index.set_index(\"cluster\",inplace=True)\n",
    "\n",
    "# print(df['cluster'].value_counts())\n",
    "# print()\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :5]: \n",
    "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "#     print(\"Cluster %d companies:\" % i, end='')\n",
    "#     for Ticker in df_index.loc[i]['Ticker'].values.tolist():\n",
    "#         print(' %s,' % Ticker, end='')\n",
    "#     print() \n",
    "#     print()\n",
    "\n",
    "pd.crosstab(df.sector, df.cluster,  margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4ed35479474592a7b22863c55e60ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='cluster', max=9), Output()), _dom_classes=('widget-interâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wordcloud import WordCloud \n",
    "import matplotlib.pyplot as plt  \n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def wc(cluster):\n",
    "    df_1 = df[df['cluster']==cluster]\n",
    "    \n",
    "    comment_words = ' ' \n",
    "    for val in df_1.sec_value:  \n",
    "        val = str(val) \n",
    "        tokens = val.split()  \n",
    "        for i in range(len(tokens)): \n",
    "            tokens[i] = tokens[i].lower() \n",
    "        for words in tokens: \n",
    "            comment_words = comment_words + words + ' '\n",
    "\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                    background_color ='white', \n",
    "                    stopwords = stopwords, \n",
    "                    min_font_size = 10).generate(comment_words) \n",
    "\n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(wc, cluster=(0, 9, 1))\n",
    "output = interactive_plot.children[-1]\n",
    "#output.layout.height = '500px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2nlp(cluster,k):\n",
    "    dfc = df[df['cluster']==cluster]\n",
    "    dfcList = list(dfc['sec_value'])\n",
    "\n",
    "    totalvocab_tokenized = []\n",
    "    for i in dfcList:\n",
    "        allwords_tokenized = tokenize_only(i)\n",
    "        totalvocab_tokenized.extend(allwords_tokenized)\n",
    "\n",
    "    vocab_frame_c = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_tokenized)\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=500,\n",
    "                                     min_df=0.01, stop_words=stopwords,\n",
    "                                     use_idf=True, tokenizer=tokenize_only, ngram_range=(1,3))\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(dfcList)\n",
    "\n",
    "    print(tfidf_matrix.shape)\n",
    "\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "    print(terms)\n",
    "    dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    num_clusters = k\n",
    "    km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "    km.fit(tfidf_matrix)\n",
    "\n",
    "    clusters = km.labels_.tolist()\n",
    "\n",
    "    dfc['cluster2'] = clusters\n",
    "    dfc_index = dfc.copy()\n",
    "    dfc_index.set_index(\"cluster2\",inplace=True)\n",
    "\n",
    "#     print(dfc['cluster2'].value_counts())\n",
    "    print()\n",
    "\n",
    "    print(\"Top terms per cluster:\")\n",
    "    print()\n",
    "\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "    for i in range(num_clusters):\n",
    "        print(\"Cluster %d words:\" % i, end='')\n",
    "\n",
    "        for ind in order_centroids[i, :5]: \n",
    "            print(' %s' % vocab_frame_c.loc[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "#         print(\"Cluster %d companies:\" % i, end='')\n",
    "#         for Ticker in dfc_index.loc[i]['Ticker'].values.tolist():\n",
    "#             print(' %s,' % Ticker, end='')\n",
    "#         print() \n",
    "#         print()\n",
    "    \n",
    "#     pd.crosstab([dfc.Sector,dfc.industry], dfc.cluster2, margins = True)\n",
    "    return dfc\n",
    "\n",
    "interact(t2nlp, cluster=(0,9,1), k=(1,10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_out = []\n",
    "for i in range(num_clusters):\n",
    "    dfc = t2nlp(cluster=i,k=6)\n",
    "    appended_out.append(dfc)\n",
    "\n",
    "df_out = pd.concat(appended_out)\n",
    "print(df_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t0 = df_out.groupby(['ticker','cluster'])['sec_adsh'].count().reset_index()\n",
    "df_t1 = df_out.groupby(['ticker'])['sec_adsh'].count().reset_index()\n",
    "df_t2 = pd.merge(df_t0, df_t1, on='Ticker')\n",
    "df_t3 = df_out.groupby(['ticker','cluster'])['period'].max().reset_index()\n",
    "df_t4 = pd.merge(df_t2, df_t3, on=['ticker','cluster'])\n",
    "df_t4.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "stop_words.extend(['together','collectively','provide','well','companies','company','companys','service','services','product','products','commercial','business','customer','united','states','us','u.s.','june','january','may','february','march','july','august','september','november','december','april','october','corporation','corp','incorporated','inc.','inc','including','primarily','provides','nature','also','new','used','statements','consolidated','description','include','note','one','two','three','organization'])\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  \n",
    "\n",
    "# Convert to list\n",
    "data = df.value.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# !python3 -m spacy download en  # run in terminal once\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    print()\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "# print(data_ready[0:10])\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "# print(corpus[0:10])\n",
    "\n",
    "# # Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=30, \n",
    "#                                            random_state=100,\n",
    "#                                            update_every=1,\n",
    "#                                            chunksize=10,\n",
    "#                                            passes=10,\n",
    "#                                            alpha='symmetric',\n",
    "#                                            iterations=100,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "#         print(row_list)\n",
    "#         print(ldamodel.per_word_topics)\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "print(topics)\n",
    "fig, axes = plt.subplots(10, 3, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_ready for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df_output = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df_output.loc[df_output.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df_output.loc[df_output.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df_output.loc[df_output.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Coloring of N Sentences\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def sentences_chart(lda_model=lda_model, corpus=corpus, start = 0, end = 13):\n",
    "    corp = corpus[start:end]\n",
    "    mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "    fig, axes = plt.subplots(end-start, 1, figsize=(20, (end-start)*0.95), dpi=160)       \n",
    "    axes[0].axis('off')\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i > 0:\n",
    "            corp_cur = corp[i-1] \n",
    "            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n",
    "            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n",
    "            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment='center',\n",
    "                    fontsize=16, color='black', transform=ax.transAxes, fontweight=700)\n",
    "\n",
    "            # Draw Rectange\n",
    "            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n",
    "            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n",
    "                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n",
    "\n",
    "            word_pos = 0.06\n",
    "            for j, (word, topics) in enumerate(word_dominanttopic):\n",
    "                if j < 14:\n",
    "                    ax.text(word_pos, 0.5, word,\n",
    "                            horizontalalignment='left',\n",
    "                            verticalalignment='center',\n",
    "                            fontsize=16, color=mycolors[topics],\n",
    "                            transform=ax.transAxes, fontweight=700)\n",
    "                    word_pos += .009 * len(word)  # to move the word for the next iter\n",
    "                    ax.axis('off')\n",
    "            ax.text(word_pos, 0.5, '. . .',\n",
    "                    horizontalalignment='left',\n",
    "                    verticalalignment='center',\n",
    "                    fontsize=16, color='black',\n",
    "                    transform=ax.transAxes)       \n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.suptitle('Sentence Topic Coloring for Documents: ' + str(start) + ' to ' + str(end-2), fontsize=22, y=0.95, fontweight=700)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "sentences_chart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic weights and dominant topics ------------\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(lda_model[corpus]):\n",
    "    topic_weights.append([w for i, w in row_list[0]])\n",
    "\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "# Plot the Topic Clusters using Bokeh\n",
    "output_notebook()\n",
    "n_topics = 4\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
    "              plot_width=900, plot_height=700)\n",
    "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(arr)\n",
    "print(topics)\n",
    "# df_output.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_flat)\n",
    "# print(counter)\n",
    "import random\n",
    "df_output = df_output.rename(index=str, columns={\"Industry\": \"sector\"})\n",
    "df_output[\"industry\"] = np.random.randint(1, 10, df_output.shape[0]) \n",
    "print(df_output.columns)\n",
    "print(df_output[[\"sector\", \"industry\", 'word',\"word_count\"]].head(15))\n",
    "\n",
    "# print(df_output[\"Industry\", \"Sector\", \"word_count\"].head(15))\n",
    "# out = {}\n",
    "# for i, topic in topics:\n",
    "#     list_of_words = {}\n",
    "#     for word, weight in topic:\n",
    "# #         out.append([word, i , weight, counter[word]])\n",
    "#         list_of_words[word] = counter[word]\n",
    "#     out[i] = list_of_words \n",
    "# print(out)\n",
    "# print(corpus[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = []\n",
    "for i in range(len(corpus)):\n",
    "    top_topics = lda_model.get_document_topics(corpus[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(30)]\n",
    "    train_vecs.append(topic_vec)\n",
    "print(train_vecs[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 10\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "train_vecs = np.array(train_vecs)\n",
    "tfidf_matrix = np.array(tfidf_matrix)\n",
    "train_data = np.append(np.array(tfidf_matrix_array),train_vecs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "km_both.fit(train_data)\n",
    "\n",
    "clusters = km_both.labels_.tolist()\n",
    "\n",
    "df['cluster'] = clusters\n",
    "df_index = df.copy()\n",
    "df_index.set_index(\"cluster\",inplace=True)\n",
    "\n",
    "# print(df['cluster'].value_counts())\n",
    "# print()\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "\n",
    "order_centroids = km_both.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "# for i in range(num_clusters):\n",
    "#     print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "#     for ind in order_centroids[i, :5]: \n",
    "#         print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "#     print()\n",
    "#     print()\n",
    "    \n",
    "#     print(\"Cluster %d companies:\" % i, end='')\n",
    "#     for Ticker in df_index.loc[i]['Ticker'].values.tolist():\n",
    "#         print(' %s,' % Ticker, end='')\n",
    "#     print() \n",
    "#     print()\n",
    "\n",
    "pd.crosstab(df.Sector, df.cluster,  margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_index['adsh'])\n",
    "test = str(df_index[df_index['adsh']=='0000002488-18-000189']['value'].values)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 10\n",
    "\n",
    "km_both = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km_both.fit(tfidf_matrix)\n",
    "\n",
    "print(len(km_both.labels_.tolist()))\n",
    "clusters = km_both.labels_.tolist()\n",
    "\n",
    "#cluster_frame = pd.DataFrame({'clusters': clusters})\n",
    "#print('there are ' + str(cluster_frame.shape[0]) + ' items in vocab_frame')\n",
    "\n",
    "df['cluster'] = clusters\n",
    "#df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km_both.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "df.set_index(\"cluster\")\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :5]: \n",
    "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 10\n",
    "\n",
    "km_td_idf = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km_td_idf.fit(tfidf_matrix)\n",
    "\n",
    "print(len(km_td_idf.labels_.tolist()))\n",
    "clusters = km_td_idf.labels_.tolist()\n",
    "\n",
    "#cluster_frame = pd.DataFrame({'clusters': clusters})\n",
    "#print('there are ' + str(cluster_frame.shape[0]) + ' items in vocab_frame')\n",
    "\n",
    "df['cluster'] = clusters\n",
    "#df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km_td_idf.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "df.set_index(\"cluster\")\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :5]: \n",
    "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "# #     print(\"Cluster %d companies:\" % i, end='')\n",
    "#     print(df.loc[i]['Ticker'])\n",
    "# #     print(type(df.loc[i]['Ticker']))\n",
    "#     for Ticker in df.loc[i]['Ticker'].values.tolist():\n",
    "#         print(' %s,' % Ticker, end='')\n",
    "#     print() \n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df.Sector, df.cluster,  margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heirarchachal clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import ward, dendrogram, linkage\n",
    "\n",
    "# linkage_matrix = ward(dist)\n",
    "linkage_matrix = linkage(dist, 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 20)) \n",
    "# ax = dendrogram(linkage_matrix, orientation=\"right\")\n",
    "ax = dendrogram(linkage_matrix, truncate_mode= 'lastp', p=10, orientation=\"right\", distance_sort=True)\n",
    "# plt.axvline(y=1, c='k')\n",
    "# plt.tight_layout() \n",
    "# print()\n",
    "# `\n",
    "# #uncomment below to save figure\n",
    "# #plt.savefig('ward_clusters.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linkage_matrix)\n",
    "last = linkage_matrix[-100:, 2]\n",
    "last_rev = last[::-1]\n",
    "idxs = np.arange(1, len(last) + 1)\n",
    "plt.plot(idxs, last_rev)\n",
    "\n",
    "acceleration = np.diff(last, 2)  # 2nd derivative of the distances\n",
    "acceleration_rev = acceleration[::-1]\n",
    "plt.plot(idxs[:-2] + 1, acceleration_rev)\n",
    "plt.show()\n",
    "k = acceleration_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters\n",
    "print(\"clusters:\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) \n",
    "ax = dendrogram(linkage_matrix, truncate_mode= 'lastp', p=100, orientation=\"right\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
