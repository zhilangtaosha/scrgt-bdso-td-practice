{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir('/home/jovyan/scrgt-bdso-cc-datascience/datapipeline'\n",
    "os.chdir('/Users/rayduong/Documents/GitHub/scrgt-bdso-cc-datascience/datapipeline')\n",
    "ESPORT= os.environ.get('ESPORT')\n",
    "ESHOST= os.environ.get('ESHOST')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-q8JYD36CdYr"
   },
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "colab_type": "code",
    "id": "_Qgy7Jmr5wSx",
    "outputId": "0e337fd6-d854-4e17-a74f-7b4c8d087b69"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from IPython.display import HTML\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True #OPTIONAL - to disable outputs from Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FxkL4RXwVToa"
   },
   "source": [
    "## Create sentence embeddings from Elmo via TensorFlow Hub\n",
    "https://tfhub.dev/google/elmo/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIhHrFsmOC6C"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "url = \"https://tfhub.dev/google/elmo/2\"\n",
    "embed = hub.Module(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Plot Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Directory to Data\n",
    "# read from data\n",
    "# Create elmoWeights Folder, in not exists\n",
    "# Write weights to data/emloWeights\n",
    "\n",
    "\n",
    "# datadir = os.path.join(rootdir, 'data') \n",
    "path_to_json = './data/person/themoviedb/'\n",
    "\n",
    "elmodir = os.getcwd()+'/models/'\n",
    "\n",
    "# create elmoWeights directory - if it does not exist\n",
    "if not os.path.exists(elmodir):\n",
    "    os.makedirs(elmodir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ PLOT DATA - ALL from PKL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_files = [path_to_json+pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json') and os.stat(path_to_json+pos_json).st_size>0]\n",
    "len(json_files)\n",
    "\n",
    "\n",
    "bio_dict = {}\n",
    "for f in json_files:\n",
    "    data = json.load(open(f) )\n",
    "    bio = data[\"biography\"]\n",
    "    bio_dict[data[\"_actorID\"]] = bio\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"An American actor and former professional wrestler, mixed martial artist, and bodybuilder.\\n\\nFrom 2000–2010 and 2013–2014, he was signed to WWE under the ring name Batista, where he became a six-time world champion, winning WWE's World Heavyweight Championship four times and the WWE Championship twice. He holds the record for the longest reign as World Heavyweight Champion at 282 days, and has also won the World Tag Team Championship three times (twice with Ric Flair and once with John Cena) and the WWE Tag Team Championship once (with Rey Mysterio). He was the winner of the 2005 Royal Rumble match and went on to headline WrestleMania 21, one of the top five highest-grossing pay-per-view events in professional wrestling history. When Bautista returned to WWE in 2014, he won the 2014 Royal Rumble match, after which he headlined WrestleMania XXX.\\n\\nIn August 2012, Bautista signed a contract with Classic Entertainment & Sports to fight in mixed martial arts. He won his MMA debut on October 6, 2012, defeating Vince Lucero via TKO in the first round. He was representing the Philippines during the fight. Bautista first entered the world of acting in 2006. As an actor, Bautista has starred in The Man with the Iron Fists (2012), Riddick (2013), Guardians of the Galaxy (2014), the 24th James Bond film, Spectre (2015), L.A. Slasher (2015), and in the 2016 Kickboxer reboot, Kickboxer: Vengeance. Bautista is set to reprise his role as Drax in Guardians of the Galaxy Vol. 2 (2017) and Avengers: Infinity War (2018). He has also appeared in several direct-to-video films since 2009.\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencesAll = list(bio_dict.values())\n",
    "sentencesAll[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSentences(sentences):\n",
    "    punctuation = '!\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~'  #TBD\n",
    "        #TBD - find sentence splitter - \n",
    "        # remove punctuation, strip white space; remove non-standard characters; remove numbers; \n",
    "    i = 0\n",
    "    for s in sentences:\n",
    "        s = s.strip()\n",
    "        s = s.lower().replace('\\n', ' ').replace('\\t', ' ').replace('\\xa0',' ').replace(\"[0-9]\", \" \")\n",
    "        s = s.replace(',', '').replace('.','') #TBD - replace with punctuation\n",
    "        s = s[0:280]\n",
    "        sentences[i] = s\n",
    "        i += 1\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For Pickle Input\n",
    "sentences = cleanSentences(sentencesAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a welsh-born english actor in addition to starring roles in big budget hollywood films he has long been heavily involved in films produced by independent producers and art houses bale first caught the public eye when he was cast in the starring role of steven spielberg's empire o\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test - \n",
    "sentences[99]\n",
    "# len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Sentences in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HsGkzCltOMOl",
    "outputId": "8c4dfea2-0b4d-4ad2-cc73-b7c6aab18030"
   },
   "outputs": [],
   "source": [
    "# score n Sentences at time - to pipeline this\n",
    "def defineEmbeddings (start, end):    \n",
    "    embeddings = embed(\n",
    "        sentences[start:end],\n",
    "        signature=\"default\",\n",
    "        as_dict=True)[\"default\"]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "oA6BO4a_Oswf",
    "outputId": "13433258-113e-433e-94a2-b61cb44caf13"
   },
   "outputs": [],
   "source": [
    "# score sentences\n",
    "def scoreSentences (embeddings):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        x = sess.run(embeddings)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Weights - use names based on Movie \n",
    "import pickle as pkl\n",
    "\n",
    "def saveWeights(elmoFN, x):\n",
    "    with open('./models/'+elmoFN,'wb') as f: pkl.dump(x, f)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score N sentences at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.magic import register_line_magic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score sentences: elmoBioweights0to500.pkl\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "batch_size = 500                      # Specify number of sentences per file  (was 500)\n",
    "numPlots = len(sentences)\n",
    "numfiles = math.ceil(numPlots/batch_size) \n",
    "\n",
    "start = 0\n",
    "end = batch_size\n",
    "for file in range(1,numfiles+1):\n",
    "    elmoFN = 'elmoBioweights' + str(start) + 'to' + str(end) + '.pkl'\n",
    "    print('Score sentences:', elmoFN)\n",
    "\n",
    "        # Define Embeddings, Score, and Store Weights one Batch of N at time\n",
    "    embeddings = defineEmbeddings(start, end)\n",
    "    x = scoreSentences(embeddings)\n",
    "    saveWeights(elmoFN,x)\n",
    "    start = end\n",
    "    end = end+batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Concatenate all weight arrays in elmoWeights to build\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Weights - use names based on actorID \n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def readElmoWeightFile(file):\n",
    "    with open(elmodir+file, 'rb') as f:\n",
    "        elmo_file = pkl.load(f)\n",
    "    return list(elmo_file)\n",
    "  \n",
    "def readWeights(elmodir):\n",
    "    noFiles = len([name for name in os.listdir(elmodir) ])\n",
    "    xin = readElmoWeightFile('elmoBioweights0to'+str(batch_size)+'.pkl')\n",
    "    size = batch_size\n",
    "    for x in range(1, noFiles):\n",
    "        start = x*size\n",
    "        end = start+size\n",
    "        filename = 'elmoBioweights' + str(start) + 'to' + str(end) + '.pkl'\n",
    "        print(filename)\n",
    "        xin += readElmoWeightFile(filename)\n",
    "    return xin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model and labels catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elmoBioweights0to500.pkl']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(elmodir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elmo Vector Shape 100, 1024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xin = readWeights(elmodir)\n",
    "print(\"Elmo Vector Shape {0}, {1}\".format( len(xin), len(xin[1])) )\n",
    "\n",
    "pickle.dump( bio_dict, open( \"./models/bio_dict.pkl\", \"wb\" ) )\n",
    "pickle.dump( xin, open( \"./models/elmoBioweights.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models \n",
    "\n",
    "bio_dict = pickle.load( open( './models/bio_dict.pkl', 'rb' ) )\n",
    "xin = pickle.load( open( './models/elmoBioweights.pkl', 'rb' ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Harrison Ford'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "topActors = pd.read_csv('./top100Actors.txt', \n",
    "                     sep='\\t',\n",
    "                     header=0, \n",
    "                    dtype=str)\n",
    "\n",
    "actorsDF = topActors[['Rank','First Name','Last Name']]\n",
    "actorsDF['Full Name']= actorsDF['First Name']+' '+actorsDF['Last Name']\n",
    "actorsDF = actorsDF.drop(['First Name','Last Name'], axis=1)\n",
    "\n",
    "celebrityLookup={}\n",
    "for i, row in actorsDF.iterrows():\n",
    "    celebrityLookup.update({ row['Rank'] : row['Full Name']})\n",
    "\n",
    "celebrityLookup['10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Elasticsearch to store the vertor and peform cosine similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "pickle.dump( celebrityLookup, open( \"../models/celebrityLookup.pkl\", \"wb\" ) )\n",
    "celebrityLookup = pickle.load( open( '../models/celebrityLookup.pkl', 'rb' ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "\n",
    "es= Elasticsearch([{'host':ESHOST,'port':ESPORT}])\n",
    "\n",
    "_index='bios'\n",
    "esIndex = 'http://'+ESHOST+':'+ESPORT+'/'+_index    \n",
    "_doc= 'bio'\n",
    "\n",
    "headers={\"Content-Type\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'bios'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_body = {\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"_actorID\" : {\n",
    "        \"type\" : \"keyword\"\n",
    "      },  \n",
    "      \"celebrity\" : {\n",
    "        \"type\" : \"keyword\"\n",
    "      },         \n",
    "      \"bio_vector\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": len(xin[0])\n",
    "      }\n",
    "\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "try:\n",
    "    es.indices.delete(index = _index)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "es.indices.create(index = _index, body = request_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(bio_dict.keys())\n",
    "for i, v in zip(ids, xin):\n",
    "    _id = str(i)\n",
    "    #print(_id)\n",
    "    doc = {\n",
    "        #_actorID\tactor\tage\tbirthdate\tgender\theight\n",
    "        \"_actorID\" : i,\n",
    "        \"celebrity\" : celebrityLookup[_id],\n",
    "        \"bio_vector\": [float(x) for x in v], \n",
    "        }\n",
    "    r = requests.put(esIndex+'/_doc/'+_id, headers= headers, data = json.dumps(doc))\n",
    "    print (r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.stats(index=_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(esIndex+'/_doc/1')\n",
    "pprint(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = json.loads(r.text)[\"_source\"][\"bio_vector\"]\n",
    "q={\n",
    "  \"query\": {\n",
    "    \"script_score\": {\n",
    "      \"query\" : {\n",
    "        \"match_all\" : {}\n",
    "      },\n",
    "      \"script\": {\n",
    "        \"source\": \"cosineSimilarity(params.query_vector, doc['bio_vector']) + 1.0\", \n",
    "        \"params\": {\n",
    "          \"query_vector\": vec\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "res= es.search(index=_index, body=q)\n",
    "\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[hit['_source']['_actorID'] for hit in res['hits']['hits']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDUCE DIMENSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the sentence arrays in reduced Dimensions using PCA and TSNE\n",
    "\n",
    "Use PCA and t-SNE to reduce the 1,024 dimensions which are output from ELMo down to 2 so that we can review the outputs from the model.\n",
    "\n",
    "In machine learning, dimensionality reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. By reducing the dimension of your feature space, you have fewer relationships between features to consider which can be explored and visualized easily and also you are less likely to overfit your model. https://www.datacamp.com/community/tutorials/introduction-t-sne\n",
    "\n",
    "#### Principal Component Analysis or PCA \n",
    "Principal Component Analysis (PCA)is a linear feature extraction technique. It performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. It does so by calculating the eigenvectors from the covariance matrix. The eigenvectors that correspond to the largest eigenvalues (the principal components) are used to reconstruct a significant fraction of the variance of the original data.\n",
    "\n",
    "In simpler terms, PCA combines your input features in a specific way that you can drop the least important feature while still retaining the most valuable parts of all of the features. As an added benefit, each of the new features or components created after PCA are all independent of one another.\n",
    "t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "#### t-Distributed Stochastic Neighbor Embedding (t-SNE) \n",
    "is a non-linear technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. It is extensively applied in image processing, NLP, genomic data and speech processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE - this takes a bit of time to reduce 1024 by 15316 array into 2 dimensions\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50)  # was 50\n",
    "y = pca.fit_transform(xin)\n",
    "\n",
    "# t-Distributed Stochastic Neighbor Embedding (t-SNE) - Dimensionality Reduction technique\n",
    "from sklearn.manifold import TSNE\n",
    "y = TSNE(n_components=2).fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a unified data frame (udf) with columns: MovieID, Plot, Genre, and Y-transform\n",
    "# udf = pd.DataFrame(columns=['MovieID', 'Plot', 'Genre','Y-Transform'])\n",
    "\n",
    "i=0\n",
    "row_list = []\n",
    "for key in plot_dict: \n",
    "    row_dict = {'MovieID':key,\n",
    "                'Plot':sentences[i],\n",
    "                'Genre':genre_dict[key],\n",
    "                'Y-Transform-X': y[i][0],\n",
    "                'Y-Transform-Y': y[i][1],\n",
    "               }\n",
    "    row_list.append(row_dict)\n",
    "    i += 1\n",
    "udf = pd.DataFrame(row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf.to_csv(\"UnifiedData.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf = pd.read_csv(\"../data/elmoWeightsAll/UnifiedData.csv\", dtype={'MovieID':str} )\n",
    "udf.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Elmo contextual embeddings.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
